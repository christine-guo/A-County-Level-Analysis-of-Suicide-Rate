---
title: "Models and Model Evaluations"
author: "Christine, Michael"
date: "7/29/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = T, fig.width=8, fig.height=4)
options(scipen = 0, digits = 3)  # controls base R output
if(!require('pacman')) {
  install.packages('pacman')
}
pacman::p_load(tree, rpart, randomForest, ranger, rattle, pROC, partykit, ggplot2, glmnet, lda, data.table, ISLR, dplyr, tidyverse, car)
# install a package if it does not exist already and put the package in the path (library)
# dplyr, ggplot2,tidyr
```

We first read in the merged data and remove the X, State, County, and FIPS data. Next, we drop all the NA values and split the dataset in the following manner: 75% for training and 25% for testing.

```{r}
data <- read.csv("data/merged_data.csv") %>% select(-X, -State, -County, -FIPS)
data <- data %>% drop_na()
n <- nrow(data)
set.seed(1)
train.index <- sample(n, n*3/4) # we use about 3/4 of the subjects as the training data.
data.train <- data[train.index,]
data.test <- data[-train.index, ]
```

Our first model we create is a single decision tree using all predictors available. According to the tree, the variables that are significant in predicting a county's age adjusted suicide rate are: Population, Total_Precipitation , Net_International_Migration_Rate_2010_2019, Ed3SomeCollegePct, HispanicPct2010, BlackNonHispanicPct2010, AvgHHSize, PctEmpMining, and Avg_Temperature. 

From the leftmost end node, we can see that, according to the decision tree, counties with small populations (less that 80612) tend to see the highest age adjusted suicide rates. This seems to indicated that rural communities may be at a greater risk of suicide. This may be due to the fact that rural communities have many barriers to accessing quality healthcare as well as being less educated on mental health, leading to stigma.

```{r}
fit.single.full <- tree(Age.Adjusted.Rate ~ ., data.train)

plot(fit.single.full)
title(main = "Single Tree With All Predictors")
text(fit.single.full, pretty = 0, digits = 3)
```

Next, we take the variables determined to be significant by the decision tree and create a linear model from them. According to the linear model, Net_International_Migration_Rate_2010_2019 and HispanicPct2010 are not significant at a 10% alpha level. 

Furthermore, when controlling for all other variables, a change in AvgHHSize has the largest impact on a county's age adjusted suicide rate. That is, when AvgHHSize increases by one unit, a county's age adjusted suizide rate seems to decrease by 7.87. This may be due to the fact that larger households are better equipped to offer social and emotional support to each household member, decreasing the risk of suicide.

```{r}
fit.lm <- lm(Age.Adjusted.Rate ~ Population + Total_Precipitation + Net_International_Migration_Rate_2010_2019 + Ed3SomeCollegePct + HispanicPct2010 + BlackNonHispanicPct2010 + AvgHHSize + PctEmpMining + Avg_Temperature, data.train)

summary(fit.lm)

# residual plot 
plot(fit.lm, 1, pch=16)
abline(h=0, col="blue", lwd=3)
#qqplot 
plot(fit.lm, 2) 
```

The third model we create is a random forest model. We set mtry to be 11 as we have 34 independent variables. Ntree is also set to be 500 because, as you can see in the graph below, the error seems to settle down around there.

```{r}
fit.rf <- randomForest(Age.Adjusted.Rate ~ ., data.train, mtry = 11, ntree = 500)

plot(fit.rf, main = "Random Forest: mtry = 11, ntree = 500")
```

Create LASSO model.
```{r}
# remove county and state columns 
X <- model.matrix(Age.Adjusted.Rate~., data = data.train)[,-1:-2]
y <- data.train$Age.Adjusted.Rate
# LASSO
fit <- cv.glmnet(X, y, nfolds = 10, alpha = 1) 
plot(fit)
# list all non-zero coefficients 
coef.min <- coef(fit, s="lambda.min")
# list of variables that have non-zero coefficients 
var.min <- coef.min@Dimnames[[1]][coef.min@i + 1][-1]
```



```{r}
# get subset of data with the response variable and LASSO output 
data_subset <- select(data.train, c("Age.Adjusted.Rate", var.min))
# relaxed LASSO
fit.min.lm <- lm(Age.Adjusted.Rate~., data_subset)
# begin backwards selection until all variables are significant at 0.05 level
fit.min.lm.back1 <- update(fit.min.lm, .~. - PovertyAllAgesPct)
fit.min.lm.back2 <- update(fit.min.lm.back1, .~. - AsianNonHispanicPct2010)
fit.min.lm.back3 <- update(fit.min.lm.back2, .~. - PctEmpTrans)
fit.min.lm.back4 <- update(fit.min.lm.back3, .~. - PctEmpAgriculture)
fit.min.lm.back5 <- update(fit.min.lm.back4, .~. - Ed5CollegePlusPct)
fit.min.lm.back6 <- update(fit.min.lm.back5, .~. - BlackNonHispanicPct2010)
fit.min.lm.back7 <- update(fit.min.lm.back6, .~. - Ed4AssocDegreePct)
fit.min.lm.back8 <- update(fit.min.lm.back7, .~. - PerCapitaInc)
fit.min.lm.back9 <- update(fit.min.lm.back8, .~. - Avg_Temperature)
fit.min.lm.back10 <- update(fit.min.lm.back9, .~. - PctEmpFIRE)
fit.final <- update(fit.min.lm.back10, .~. - PctEmpInformation)

Anova(fit.final)
```

Perform diagnostics on LASSO model.
```{r}
# residual plot 
plot(fit.final, 1, pch=16)
abline(h=0, col="blue", lwd=3)
#qqplot 
plot(fit.final, 2) 
```

Now that we have created all four models, it is time to evaluate and compare them. After running the models on the testing dataset and extracting their mean squared errors, we can see that random forest formed the best, with a mean squared error of only 15.4. On the other hand, the testing error for the linear model based off of our tree's significant variables performed the worst, with a testing error of 25.8. Lasso and the single tree did not perform much better, with a testing error of 25.2 and 24.8 respectively. Although the lasso model did not perform that well, we still think it is important as it provides us with a clear and informative linear relationship between the variables.

```{r}
test.error.tree <- mean((predict(fit.single.full, data.test) - data.test$Age.Adjusted.Rate)^2)
test.error.lm <- mean((predict(fit.lm, data.test) - data.test$Age.Adjusted.Rate)^2)
test.error.lasso <- mean((predict(fit.final, data.test) - data.test$Age.Adjusted.Rate)^2)
test.error.rf <- mean((predict(fit.rf, data.test) - data.test$Age.Adjusted.Rate)^2)

data.frame(test.error.lm = test.error.lm, 
           test.error.tree = test.error.tree,
           test.error.lasso = test.error.lasso,
           test.error.rf = test.error.rf)
```
